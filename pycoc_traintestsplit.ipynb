{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__author__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
     ]
    }
   ],
   "source": [
    "import pycocotools\n",
    "print(dir(pycocotools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a hack of some labelbox code for generating a tf record training and test set\n",
    "#for some reason functionality has been removed from the site...\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def from_json(labeled_data , datadir, label_format='XY' , split = .25):\n",
    "    \"Writes labelbox JSON export into MS COCO format.\"\n",
    "    # read labelbox JSON output\n",
    "    with open(labeled_data, 'r') as file_handle:\n",
    "        label_data = json.loads(file_handle.read())\n",
    "    # setup COCO dataset container and info\n",
    "    samples = len( label_data)\n",
    "    print(samples)\n",
    "    files ={}\n",
    "    dataset = {'train': label_data[0:int((1-split)*samples)] , 'test':label_data[int((1-split)*samples):] }\n",
    "    for datatype in dataset:\n",
    "        coco = make_coco_metadata(label_data[0]['Project Name'], label_data[0]['Created By'],) \n",
    "        for data in dataset[datatype]:\n",
    "            add_label(coco, datadir, data['ID'], data['Labeled Data'], data['Label'], label_format)\n",
    "        print(datatype)\n",
    "        print(len(dataset[datatype]))\n",
    "        with open(labeled_data+datatype+'.json', 'w+') as file_handle:\n",
    "            file_handle.write(json.dumps(coco))\n",
    "        files[datatype] = labeled_data+datatype+'.json'\n",
    "    return files\n",
    "\n",
    "\n",
    "def make_coco_metadata(project_name: str, created_by: str) -> Dict[str, Any]:\n",
    "    \"\"\"Initializes COCO export data structure.\n",
    "    Args:\n",
    "        project_name: name of the project\n",
    "        created_by: email of the project creator\n",
    "    Returns:\n",
    "        The COCO export represented as a dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'info': {\n",
    "            'year': dt.datetime.now(dt.timezone.utc).year,\n",
    "            'version': None,\n",
    "            'description': project_name,\n",
    "            'contributor': created_by,\n",
    "            'url': 'labelbox.com',\n",
    "            'date_created': dt.datetime.now(dt.timezone.utc).isoformat()\n",
    "        },\n",
    "        'images': [],\n",
    "        'annotations': [],\n",
    "        'licenses': [],\n",
    "        'categories': []\n",
    "    }\n",
    "\n",
    "\n",
    "def add_label(\n",
    "        coco: Dict[str, Any], datadir, label_id: str, image_url: str,\n",
    "        labels: Dict[str, Any], label_format: str ):\n",
    "    \"\"\"Incrementally updates COCO export data structure with a new label.\n",
    "    Args:\n",
    "        coco: The current COCO export, will be incrementally updated by this method.\n",
    "        label_id: ID for the instance to write\n",
    "        image_url: URL to download image file from\n",
    "        labels: Labelbox formatted labels to use for generating annotation\n",
    "        label_format: Format of the labeled data. Valid options are: \"WKT\" and\n",
    "                      \"XY\", default is \"WKT\".\n",
    "    Returns:\n",
    "        The updated COCO export represented as a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    image = {\n",
    "        \"id\": label_id,\n",
    "        \"file_name\": image_url,\n",
    "        \"license\": None,\n",
    "        \"flickr_url\": image_url,\n",
    "        \"coco_url\": image_url,\n",
    "        \"date_captured\": None,\n",
    "    }\n",
    "    image['width'], image['height'] = Image.open(datadir+image_url).size\n",
    "    coco['images'].append(image)\n",
    "    # remove classification labels (Skip, etc...)\n",
    "    if not callable(getattr(labels, 'keys', None)):\n",
    "        return\n",
    "\n",
    "    # convert label to COCO Polygon format\n",
    "    for category_name, label_data in labels.items():\n",
    "        try:\n",
    "            # check if label category exists in 'categories' field\n",
    "            category_id = [c['id']\n",
    "                           for c in coco['categories']\n",
    "                           if c['supercategory'] == category_name][0]\n",
    "        except IndexError:\n",
    "            category_id = len(coco['categories']) + 1\n",
    "            category = {\n",
    "                'supercategory': category_name,\n",
    "                'id': category_id,\n",
    "                'name': category_name\n",
    "            }\n",
    "            coco['categories'].append(category)\n",
    "\n",
    "        polygons = _get_polygons(label_format, label_data)\n",
    "        _append_polygons_as_annotations(coco, image, category_id, polygons)\n",
    "\n",
    "\n",
    "def _append_polygons_as_annotations(coco, image, category_id, polygons):\n",
    "    \"Adds `polygons` as annotations in the `coco` export\"\n",
    "    for polygon in polygons:\n",
    "        segmentation = []\n",
    "        for x_val, y_val in polygon.exterior.coords:\n",
    "            segmentation.extend([x_val, y_val])\n",
    "\n",
    "        annotation = {\n",
    "            \"id\": len(coco['annotations']) + 1,\n",
    "            \"image_id\": image['id'],\n",
    "            \"category_id\": category_id,\n",
    "            \"segmentation\": [segmentation],\n",
    "            \"area\": polygon.area,  # float\n",
    "            \"bbox\": [polygon.bounds[0], polygon.bounds[1],\n",
    "                     polygon.bounds[2] - polygon.bounds[0],\n",
    "                     polygon.bounds[3] - polygon.bounds[1]],\n",
    "            \"iscrowd\": 0\n",
    "        }\n",
    "        \n",
    "        coco['annotations'].append(annotation)\n",
    "\n",
    "\n",
    "def _get_polygons(label_format, label_data):\n",
    "    \"Converts segmentation `label: String!` into polygons\"\n",
    "    if label_format == 'WKT':\n",
    "        if isinstance(label_data, list):  # V3\n",
    "            polygons = map(lambda x: wkt.loads(x['geometry']), label_data)\n",
    "        else:  # V2\n",
    "            polygons = wkt.loads(label_data)\n",
    "    elif label_format == 'XY':\n",
    "        polygons = []\n",
    "        for xy_list in label_data:\n",
    "            if 'geometry' in xy_list:  # V3\n",
    "                xy_list = xy_list['geometry']\n",
    "\n",
    "                # V2 and V3\n",
    "                if not isinstance(xy_list, list):\n",
    "                    LOGGER.warning('Could not get an point list to construct polygon, skipping')\n",
    "                    continue\n",
    "            else:  # V2, or non-list\n",
    "                if not isinstance(xy_list, list) or not xy_list or 'x' not in xy_list[0]:\n",
    "                    # skip non xy lists\n",
    "                    LOGGER.warning('Could not get an point list to construct polygon, skipping')\n",
    "                    continue\n",
    "\n",
    "            if len(xy_list) > 2:  # need at least 3 points to make a polygon\n",
    "                polygons.append(Polygon(map(lambda p: (p['x'], p['y']), xy_list)))\n",
    "    else:\n",
    "        exc = UnknownFormatError(label_format=label_format)\n",
    "        LOGGER.exception(exc.message)\n",
    "        raise exc\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "train\n",
      "157\n",
      "test\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "#train test split percentage\n",
    "split = .25\n",
    "json_name='../object-detection/cocofilesv2/export-2019-06-17T08_32_09.254Z_loacl.json'# change here\n",
    "datadir = '../object-detection/cocofilesv2/'# change here\n",
    "files = from_json(json_name, datadir, label_format='XY' , split = split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#generate two sep tfrecord files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (create_coco_tf_record.py, line 274)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/dataset_tools/create_coco_tf_record.py\"\u001b[0;36m, line \u001b[0;32m274\u001b[0m\n\u001b[0;31m    \"\"\"_create_tf_record_from_coco_annotations(\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from object_detection.dataset_tools import create_coco_tf_record\n",
    "\n",
    "for datatype in files:\n",
    "    create_coco_tf_record._create_tf_record_from_coco_annotations(annotations_file=files[datatype], image_dir=datadir , output_path=files[datatype]+'.record' , include_masks= False, num_shards=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
